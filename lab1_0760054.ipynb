{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab1-0760054.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsanzxc456/NLP/blob/master/lab1_0760054.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsQ_pHO3cG1m",
        "colab_type": "text"
      },
      "source": [
        "#Recommend Similar News Articles\n",
        "This notebook demonstrates how to use bag-of-word vectors and cosine similarity for news article recommendation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B80ijUN40QPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import math\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import string\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJH61jnWXdsh",
        "colab_type": "text"
      },
      "source": [
        "#Fetching the Corpus\n",
        "`get_corpus()` reads the CSV file, and **removes punctuation and stopwords** immediately after the data is accquired, then **return a copy of the list of the news contents with the cased characters converted to lowercase**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIyxZwqGVL5n",
        "colab_type": "text"
      },
      "source": [
        "`get_corpus_title()` reads the CSV file, and the return the title list of data for showing the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiznCksm-1xW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_stopwords(document):\n",
        "  stopwords_list = [\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"you're\",\"you've\",\"you'll\",\"you'd\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"she's\",\"her\",\"hers\",\"herself\",\"it\",\"it's\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"that'll\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\"on\",\"off\",\"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"most\",\"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"don't\",\"should\",\"should've\",\"now\",\"d\",\"ll\",\"m\",\"o\",\"re\",\"ve\",\"y\",\"ain\",\"aren\",\"aren't\",\"couldn\",\"couldn't\",\"didn\",\"didn't\",\"doesn\",\"doesn't\",\"hadn\",\"hadn't\",\"hasn\",\"hasn't\",\"haven\",\"haven't\",\"isn\",\"isn't\",\"ma\",\"mightn\",\"mightn't\",\"mustn\",\"mustn't\",\"needn\",\"needn't\",\"shan\",\"shan't\",\"shouldn\",\"shouldn't\",\"wasn\",\"wasn't\",\"weren\",\"weren't\",\"won\",\"won't\",\"wouldn\",\"wouldn't\"]\n",
        "  doc_words = document.split()\n",
        "  resultwords  = [word for word in doc_words if word not in stopwords_list]\n",
        "  result = ' '.join(resultwords)\n",
        "  return result"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwPf9e26O9sn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_corpus():\n",
        "  df = pd.read_csv('https://raw.githubusercontent.com/bshmueli/108-nlp/master/reuters.csv') # https://bit.ly/nlp-reuters\n",
        "  corpus = df.content.to_list()\n",
        "  corpus_punc_removed = corpus\n",
        "  corpus_stopwords_removed = corpus\n",
        "  for i in range(0,len(corpus)):\n",
        "    corpus_punc_removed[i] = corpus[i].translate(str.maketrans('', '', string.punctuation)).lower()\n",
        "    corpus_stopwords_removed[i] = remove_stopwords(corpus_punc_removed[i])\n",
        "  return corpus_stopwords_removed"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPFSBdYhZFZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_corpus_title():\n",
        "  df = pd.read_csv('https://raw.githubusercontent.com/bshmueli/108-nlp/master/reuters.csv') # https://bit.ly/nlp-reuters\n",
        "  corpus_title = df.title.to_list()\n",
        "  return corpus_title"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4lh_w1HXU14",
        "colab_type": "text"
      },
      "source": [
        "#Computing word frequencies\n",
        "`get_vocab_from_document(document)` computes the word frequencies **in a single document** given. It returns a list of 2-tuples. Each tuple contains the token and its frequency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ADOHLlkWAgB",
        "colab_type": "text"
      },
      "source": [
        "`get_vocab_from_corpus(corpus)` computes the word frequencies **in the whole corpus** given. It returns a list of 2-tuples. Each tuple contains the token and its frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arntcI3OTHTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(document):\n",
        "  words = document.split(' ')\n",
        "  return words"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJvtr3VZLkAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vocab_from_corpus(corpus):\n",
        "  vocabulary = Counter()\n",
        "  for document in corpus:\n",
        "    tokens = tokenize(document)\n",
        "    vocabulary.update(tokens)\n",
        "  return vocabulary"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYRSljT_cjR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vocab_from_document(document):\n",
        "  vocabulary = Counter()\n",
        "  tokens = tokenize(document)\n",
        "  vocabulary.update(tokens)\n",
        "  return vocabulary"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwy_Cb-9cYwe",
        "colab_type": "text"
      },
      "source": [
        "#Compute TFIDF Score and return as a Vector\n",
        "`get_df(word_list)` compute df of each word in given word_list, then return as a list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM3hfj5sWRCN",
        "colab_type": "text"
      },
      "source": [
        "`tfidf_score(num,key_word,document)` returns a TFIDF score for word `key_word` in document `document`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0x6ZGSYWtVt",
        "colab_type": "text"
      },
      "source": [
        "`tfidf_vector(document)` returns a TFIDF vector with element in `top_1000_vocab`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6X2fT2H8PAx",
        "colab_type": "text"
      },
      "source": [
        "`cosine_similarity` compute cosine similarity between two numerical vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3vL0wWJHqLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_df(word_list):\n",
        "  df_list = []\n",
        "  for word in word_list:\n",
        "    tmp_df = 0\n",
        "    for i in range(0,len(corpus)):\n",
        "      each_doc_vocab = get_vocab_from_document(corpus[i])\n",
        "      if each_doc_vocab[word[0]]!=0:\n",
        "        tmp_df = tmp_df + 1\n",
        "    # print(\"'{}' has ({})\".format(word[0],tmp_df))\n",
        "    df_list.append(tmp_df)\n",
        "  return df_list\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E--cW87NpxSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tfidf_score(num,key_word,document):\n",
        "  doc_vocab = get_vocab_from_document(document)\n",
        "  tf = doc_vocab[key_word]/len(list(doc_vocab.elements()))\n",
        "  idf = math.log10(len(corpus)/df_list[num])\n",
        "  tfidf = tf * idf\n",
        "  # print(\"tfidf of '{}' is ({})\".format(key_word,tfidf))\n",
        "  return tfidf"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z46ozBlhgh9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tfidf_vector(document):\n",
        "  tfidf_vec = []\n",
        "  word_num = 0\n",
        "  for word in top_1000_vocab:   \n",
        "    tfidf_vec.append(tfidf_score(word_num,word[0],document))\n",
        "    word_num += 1\n",
        "    \n",
        " \n",
        "  return tfidf_vec"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WuWEBWLQBAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine_similarity(vec_a, vec_b):\n",
        "  assert len(vec_a) == len(vec_b)\n",
        "  if sum(vec_a) == 0 or sum(vec_b) == 0:\n",
        "    return 0 # hack\n",
        "  a_b = sum(i[0] * i[1] for i in zip(vec_a, vec_b))\n",
        "  a_2 = sum([i*i for i in vec_a])\n",
        "  b_2 = sum([i*i for i in vec_b])\n",
        "  return a_b/(math.sqrt(a_2) * math.sqrt(b_2))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FEyKg0mkeLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doc_similarity(doc_a, doc_b):\n",
        "  return cosine_similarity(tfidf_vector(doc_a), tfidf_vector(doc_b))\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddRwu-O1f13Q",
        "colab_type": "text"
      },
      "source": [
        "# Find Similar Documents\n",
        "Find and print the $k$ most similar titles to a given content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6rIkWUrmhxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def k_similar(seed_id, k):\n",
        "  seed_doc = corpus[seed_id]\n",
        "  seed_title = corpus_title[seed_id]\n",
        "  print('> \"{}\"'.format(seed_title))\n",
        "  similarities = [doc_similarity(seed_doc, doc) for id, doc in enumerate(corpus)]\n",
        "  top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i])[-k:] # https://stackoverflow.com/questions/13070461/get-indices-of-the-top-n-values-of-a-list\n",
        "  nearest = [[corpus_title[id], similarities[id]] for id in top_indices]\n",
        "  print()\n",
        "  for story in reversed(nearest):\n",
        "    print('* \"{}\" ({})'.format(story[0], story[1]))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgPZe4rxUVPQ",
        "colab_type": "text"
      },
      "source": [
        "# Test our program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjW8MQXZUmJU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "d71821fe-f245-44fd-acc1-dac1a6133698"
      },
      "source": [
        "corpus = get_corpus()\n",
        "corpus_title = get_corpus_title()\n",
        "top_1000_vocab = get_vocab_from_corpus(corpus).most_common(1000)\n",
        "df_list = get_df(top_1000_vocab)\n",
        "k_similar(54, 5)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> \"British police name suicide bomber, threat level raised to critical\"\n",
            "\n",
            "* \"British police name suicide bomber, threat level raised to critical\" (1.0)\n",
            "* \"Trump condemns leaks after UK police briefly halt information sharing\" (0.6285046844403149)\n",
            "* \"Two days from UK election, security dominates campaign after London attack\" (0.6240940381380864)\n",
            "* \"Middle-aged London attacker was criminal who wasn’t seen as threat\" (0.605716007450487)\n",
            "* \"’Enough is enough’ PM May says after London attackers kill seven\" (0.6033688132049058)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}