{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab3-0760054.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsanzxc456/NLP/blob/master/lab3_0760054.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH7hBBfP3ssD",
        "colab_type": "text"
      },
      "source": [
        "# Part 1\n",
        "Build a 2-gram model for the Twitter train data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HlI_7FQXftH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re, math\n",
        "from collections import Counter, defaultdict\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmFDIurQ3yNB",
        "colab_type": "text"
      },
      "source": [
        "## Fetching train data\n",
        "First download the train tweet data, then tokenize the data converted to lowercase.\n",
        "\n",
        "Take frequent terms which appearing more than 3 times as vocabulary, and replace the infrequent term as 'UNK'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzqbiEcVgdQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweet_tokenizer = TweetTokenizer()\n",
        "corpus = pd.read_json(\"https://raw.githubusercontent.com/bshmueli/108-nlp/master/tweets_train.txt\",lines=True).values.tolist()\n",
        "corpus_vocab = Counter()\n",
        "for i in range(0,len(corpus)):\n",
        "  corpus[i] =  ['<s>'] + tweet_tokenizer.tokenize(corpus[i][0].lower()) + ['</s>']\n",
        "  corpus_vocab.update(corpus[i])\n",
        "\n",
        "for word in list(corpus_vocab):\n",
        "  if corpus_vocab[word] < 2:\n",
        "    del corpus_vocab[word]\n",
        "\n",
        "for i in range(0,len(corpus)):\n",
        "  corpus[i] = [(token if token in corpus_vocab else '<UNK>') for token in corpus[i]]"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ5b1bBb33s4",
        "colab_type": "text"
      },
      "source": [
        "## Building bigrams for train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CuokI4TAkI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_bigrams_counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "corpus_bigrams = []\n",
        "for i in range(0,len(corpus)):\n",
        "  corpus_bigrams += list(nltk.bigrams(corpus[i]))\n",
        "  for w1,w2 in nltk.bigrams(corpus[i]):\n",
        "    corpus_bigrams_counts[w1][w2] += 1\n",
        "# corpus_bigrams_counts"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSI3disd39dO",
        "colab_type": "text"
      },
      "source": [
        "## Fetching test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsFV_VSZgN4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_corpus = pd.read_json(\"https://raw.githubusercontent.com/bshmueli/108-nlp/master/tweets_test.txt\",lines=True).values.tolist()\n",
        "for i in range(0,len(test_corpus)):\n",
        "  test_corpus[i] =  ['<s>'] + tweet_tokenizer.tokenize(test_corpus[i][0].lower()) + ['</s>']\n",
        "  test_corpus[i] = [(token if token in corpus_vocab else '<UNK>') for token in test_corpus[i]]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF9FcTOe4DMm",
        "colab_type": "text"
      },
      "source": [
        "## Building bigrams for test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWCqxu9wSU_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_bigrams = []\n",
        "for i in range(0,len(test_corpus)):\n",
        "  test_bigrams += list(nltk.bigrams(test_corpus[i]))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgcZPywz4Hwh",
        "colab_type": "text"
      },
      "source": [
        "## Computing the perplexity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRjrvy2FZDu1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "10f54d1e-01f7-46e9-a36e-d74522673856"
      },
      "source": [
        "probabilities_train = [(1 + corpus_bigrams_counts[w1][w2])/(len(corpus_vocab) + sum(corpus_bigrams_counts[w1].values())) for w1, w2 in corpus_bigrams]\n",
        "N = len(corpus_bigrams)\n",
        "cross_entropy_train =-1/N * sum([math.log(p, 2) for p in probabilities_train])\n",
        "print('Average perplexity for train data tweet is {:.3f}'.format(math.pow(2, cross_entropy_train)))\n",
        "print('---')\n",
        "probabilities_test = [(1 + corpus_bigrams_counts[w1][w2])/(len(corpus_vocab) + sum(corpus_bigrams_counts[w1].values())) for w1, w2 in test_bigrams]\n",
        "N = len(test_bigrams)\n",
        "cross_entropy_test =-1/N * sum([math.log(p, 2) for p in probabilities_test])\n",
        "print('Average perplexity for test data tweet is {:.3f}'.format(math.pow(2, cross_entropy_test)))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average perplexity for train data tweet is 1387.291\n",
            "---\n",
            "Average perplexity for test data tweet is 1605.826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Fj6-326Uh0",
        "colab_type": "text"
      },
      "source": [
        "# Part 2\n",
        "Build a bi-directional 2-gram model by training on the Twitter train data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fvm7EwPT6XCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re, math\n",
        "from collections import Counter, defaultdict\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEkeH6Tf6ZUA",
        "colab_type": "text"
      },
      "source": [
        "## Fetching train data\n",
        "First download the train tweet data, then tokenize the data converted to lowercase.\n",
        "\n",
        "Take frequent terms which appearing more than 3 times as vocabulary, and replace the infrequent term as 'UNK'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LewVRWh6b36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweet_tokenizer = TweetTokenizer()\n",
        "forward_corpus = pd.read_json(\"https://raw.githubusercontent.com/bshmueli/108-nlp/master/tweets_train.txt\",lines=True).values.tolist()\n",
        "corpus_vocab = Counter()\n",
        "for i in range(0,len(forward_corpus)):\n",
        "  forward_corpus[i] =  ['<s>'] + tweet_tokenizer.tokenize(forward_corpus[i][0].lower()) + ['</s>']\n",
        "  corpus_vocab.update(forward_corpus[i])\n",
        "\n",
        "for word in list(corpus_vocab):\n",
        "  if corpus_vocab[word] < 2:\n",
        "    del corpus_vocab[word]\n",
        "\n",
        "for i in range(0,len(forward_corpus)):\n",
        "  forward_corpus[i] = [(token if token in corpus_vocab else '<UNK>') for token in forward_corpus[i]]\n",
        "\n",
        "backward_corpus = [forward_corpus[i][::-1] for i in range(0,len(forward_corpus))]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhQO9zZM6efO",
        "colab_type": "text"
      },
      "source": [
        "## Building bigrams for train data with both directions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtFS2M4z6hui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "forward_corpus_bigrams_counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "forward_corpus_bigrams = []\n",
        "for i in range(0,len(forward_corpus)):\n",
        "  forward_corpus_bigrams += list(nltk.bigrams(forward_corpus[i]))\n",
        "  for w1,w2 in nltk.bigrams(forward_corpus[i]):\n",
        "    forward_corpus_bigrams_counts[w1][w2] += 1\n",
        "\n",
        "backward_corpus_bigrams_counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "backward_corpus_bigrams = []\n",
        "for i in range(0,len(backward_corpus)):\n",
        "  backward_corpus_bigrams += list(nltk.bigrams(backward_corpus[i]))\n",
        "  for w1,w2 in nltk.bigrams(backward_corpus[i]):\n",
        "    backward_corpus_bigrams_counts[w1][w2] += 1"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws0akUrj6jwa",
        "colab_type": "text"
      },
      "source": [
        "## Fetching test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KKHx5KB6kRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_corpus = pd.read_json(\"https://raw.githubusercontent.com/bshmueli/108-nlp/master/tweets_test.txt\",lines=True).values.tolist()\n",
        "for i in range(0,len(test_corpus)):\n",
        "  test_corpus[i] =  ['<s>'] + tweet_tokenizer.tokenize(test_corpus[i][0].lower()) + ['</s>']\n",
        "  test_corpus[i] = [(token if token in corpus_vocab else '<UNK>') for token in test_corpus[i]]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0HhJC2r6mnY",
        "colab_type": "text"
      },
      "source": [
        "## Building bigrams for test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VksM2DJG6pO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_bigrams = []\n",
        "for i in range(0,len(test_corpus)):\n",
        "  test_bigrams += list(nltk.bigrams(test_corpus[i]))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPVRqz9O6p3w",
        "colab_type": "text"
      },
      "source": [
        "## Computing the perplexity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lh8wiyQ6rjH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "8809e97a-e75a-499c-c340-5e0ead2f5e15"
      },
      "source": [
        "forward_probabilities_test = [(1 + forward_corpus_bigrams_counts[w1][w2])/(len(corpus_vocab) + sum(forward_corpus_bigrams_counts[w1].values())) for w1, w2 in test_bigrams]\n",
        "backward_probabilities_test = [(1 + backward_corpus_bigrams_counts[w1][w2])/(len(corpus_vocab) + sum(backward_corpus_bigrams_counts[w1].values())) for w1, w2 in test_bigrams]\n",
        "N = len(test_bigrams)\n",
        "for r in range(0,105,5):\n",
        "  r_factor = r/100\n",
        "  mix_probabilities_test = [(forward_probabilities_test[i]*r_factor + backward_probabilities_test[i]*(1-r_factor)) for i in range(0,len(forward_probabilities_test))]\n",
        "  cross_entropy_test =-1/N * sum([math.log(p, 2) for p in mix_probabilities_test])\n",
        "  print('Average perplexity with r = {:.2f} is {:.3f}'.format(r_factor,math.pow(2, cross_entropy_test)))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average perplexity with r = 0.00 is 14187.501\n",
            "Average perplexity with r = 0.05 is 5985.706\n",
            "Average perplexity with r = 0.10 is 4670.248\n",
            "Average perplexity with r = 0.15 is 3960.347\n",
            "Average perplexity with r = 0.20 is 3493.070\n",
            "Average perplexity with r = 0.25 is 3154.291\n",
            "Average perplexity with r = 0.30 is 2893.944\n",
            "Average perplexity with r = 0.35 is 2685.874\n",
            "Average perplexity with r = 0.40 is 2514.828\n",
            "Average perplexity with r = 0.45 is 2371.202\n",
            "Average perplexity with r = 0.50 is 2248.598\n",
            "Average perplexity with r = 0.55 is 2142.569\n",
            "Average perplexity with r = 0.60 is 2049.918\n",
            "Average perplexity with r = 0.65 is 1968.292\n",
            "Average perplexity with r = 0.70 is 1895.921\n",
            "Average perplexity with r = 0.75 is 1831.472\n",
            "Average perplexity with r = 0.80 is 1773.942\n",
            "Average perplexity with r = 0.85 is 1722.617\n",
            "Average perplexity with r = 0.90 is 1677.087\n",
            "Average perplexity with r = 0.95 is 1637.421\n",
            "Average perplexity with r = 1.00 is 1605.826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mTG36Z_6umR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "cc5e74d7-c7ee-44b9-b95e-be22529abbda"
      },
      "source": [
        "print('r = 1.00 minimize the perplexity of the Twitter test data')\n",
        "r_factor = 1.00\n",
        "mix_probabilities_test = [(forward_probabilities_test[i]*r_factor + backward_probabilities_test[i]*(1-r_factor)) for i in range(0,len(forward_probabilities_test))]\n",
        "cross_entropy_test =-1/N * sum([math.log(p, 2) for p in mix_probabilities_test])\n",
        "print('Average perplexity for test data tweet is {:.3f}'.format(math.pow(2, cross_entropy_test)))\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r = 1.00 minimize the perplexity of the Twitter test data\n",
            "Average perplexity for test data tweet is 1605.826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GIFCeFe6wRs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57ebf2ef-50f4-45d0-8174-f3fb3ac71147"
      },
      "source": [
        "forward_probabilities_train = [(1 + forward_corpus_bigrams_counts[w1][w2])/(len(corpus_vocab) + sum(forward_corpus_bigrams_counts[w1].values())) for w1, w2 in forward_corpus_bigrams]\n",
        "backward_probabilities_train = [(1 + backward_corpus_bigrams_counts[w1][w2])/(len(corpus_vocab) + sum(backward_corpus_bigrams_counts[w1].values())) for w1, w2 in forward_corpus_bigrams]\n",
        "mix_probabilities_train = [(forward_probabilities_train[i]*r_factor + backward_probabilities_train[i]*(1-r_factor)) for i in range(0,len(forward_probabilities_train))]\n",
        "N = len(forward_corpus_bigrams)\n",
        "cross_entropy_train =-1/N * sum([math.log(p, 2) for p in mix_probabilities_train])\n",
        "print('Average perplexity for train data tweet is {:.3f}'.format(math.pow(2, cross_entropy_train)))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average perplexity for train data tweet is 1387.291\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}