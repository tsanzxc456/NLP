{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsanzxc456/NLP/blob/master/final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qExTQuMNcH_",
        "colab_type": "text"
      },
      "source": [
        "# Final Project of NLP-109 : **#SocialNLP EmotionGIF 2020 Challenge**[(website link)](https://sites.google.com/view/emotiongif-2020/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G16xmDhpuwA_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "dda4055e-10b5-4efd-e93a-7ceda5eedb10"
      },
      "source": [
        "import tensorflow as tf\n",
        "if tf.__version__ != '1.15.0':\n",
        "  !pip uninstall tensorflow==2.2.0\n",
        "  !pip install tensorflow==1.15.0\n",
        "import os\n",
        "import collections\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime\n",
        "\n",
        "!pip install bert-tensorflow\n",
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization\n",
        "from bert import modeling"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 10kB 16.1MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 30kB 3.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 40kB 3.9MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 51kB 3.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 61kB 3.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFAsoyRTON5U",
        "colab_type": "text"
      },
      "source": [
        "## import BERT pre-trained models\n",
        "#### *Note : upload uncased_L-12_H-768_A-12 manually first !!*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0luiCIk0jmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BERT_VOCAB= './uncased_L-12_H-768_A-12/vocab.txt'\n",
        "BERT_INIT_CHKPNT = './uncased_L-12_H-768_A-12/bert_model.ckpt'\n",
        "BERT_CONFIG = './uncased_L-12_H-768_A-12/bert_config.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP-b1mwoIeyZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "4fbe6c06-9e65-441e-b8f1-e162a5489346"
      },
      "source": [
        "tokenization.validate_case_matches_checkpoint(True,BERT_INIT_CHKPNT)\n",
        "tokenizer = tokenization.FullTokenizer(\n",
        "      vocab_file=BERT_VOCAB, do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHrLlLv_OyH2",
        "colab_type": "text"
      },
      "source": [
        "## Fetch the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q1q3TPIIoaG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "764e2bac-034f-486d-ff55-6876861cfa41"
      },
      "source": [
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"https://nctu_team_yellow.imfast.io/nlp_final_project/train_gold.csv\", delimiter=',', header=None, names=['idx', 'text', 'reply', 'categories', 'mp4'])\n",
        "\n",
        "idx_list = df.idx.tolist()\n",
        "text_list = df.text.tolist()\n",
        "reply_list = df.reply.tolist()\n",
        "categories_list = df.categories.tolist()\n",
        "\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "df.head(5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 32,000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>text</th>\n",
              "      <th>reply</th>\n",
              "      <th>categories</th>\n",
              "      <th>mp4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>we can all agree that any song by Niall Horan.</td>\n",
              "      <td>oui oui</td>\n",
              "      <td>[\"yes\"]</td>\n",
              "      <td>6dc39e96b11275f064fdaed88273b45e.mp4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Will you be installing #ScottyFromMarketing's ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[\"no\"]</td>\n",
              "      <td>cfff051f05d8d3b7136c7d58ea6ad55f.mp4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Growing up my mum would call me a Nigga despit...</td>\n",
              "      <td>And he joins in??? Pour some hot grits on em</td>\n",
              "      <td>[\"smh\"]</td>\n",
              "      <td>bf39e7bd9ad24354ce3ba6822b0104af.mp4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Rest your head on my chest when the world feel...</td>\n",
              "      <td>üòÇüòÇüòÇüòÇüòÇ</td>\n",
              "      <td>[\"wink\"]</td>\n",
              "      <td>173a707a04c277354a2f23cf01d6151e.mp4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Imagine Will Hernandez and Wills both doing a ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[\"yes\"]</td>\n",
              "      <td>aab6d6bfb0c1382269ddba9b71cc8b7a.mp4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   idx  ...                                   mp4\n",
              "0    0  ...  6dc39e96b11275f064fdaed88273b45e.mp4\n",
              "1    1  ...  cfff051f05d8d3b7136c7d58ea6ad55f.mp4\n",
              "2    2  ...  bf39e7bd9ad24354ce3ba6822b0104af.mp4\n",
              "3    3  ...  173a707a04c277354a2f23cf01d6151e.mp4\n",
              "4    4  ...  aab6d6bfb0c1382269ddba9b71cc8b7a.mp4\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2nr3hLQPuQL",
        "colab_type": "text"
      },
      "source": [
        "## Create a training data as panda dataframe with `idx`, `text_reply`, `categories`\n",
        "#### Combine text and reply as a new sentence.\n",
        "#### Convert the training labels to list of number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlKXMiUswdCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_reply_list = text_list\n",
        "for i in range(0,len(text_list)):\n",
        "  if type(reply_list[i]) == str:\n",
        "    text_reply_list[i] = text_list[i] + reply_list[i]\n",
        "\n",
        "categories = ['\"agree\"', '\"applause\"', '\"awww\"', '\"dance\"', '\"deal_with_it\"', '\"do_not_want\"', '\"eww\"', '\"eye_roll\"', '\"facepalm\"', '\"fist_bump\"', '\"good_luck\"', '\"happy_dance\"', '\"hearts\"', '\"high_five\"', '\"hug\"', '\"idk\"', '\"kiss\"', '\"mic_drop\"', '\"no\"', '\"oh_snap\"', '\"ok\"', '\"omg\"', '\"oops\"', '\"please\"', '\"popcorn\"', '\"scared\"', '\"seriously\"', '\"shocked\"', '\"shrug\"', '\"sigh\"', '\"slow_clap\"', '\"smh\"', '\"sorry\"', '\"thank_you\"', '\"thumbs_down\"', '\"thumbs_up\"', '\"want\"', '\"win\"', '\"wink\"', '\"yawn\"', '\"yes\"', '\"yolo\"', '\"you_got_this\"']\n",
        "cate_labels_list = [[] for i in range(43)]\n",
        "for i in range(0,len(categories)):\n",
        "  for idx in range(0,len(categories_list)):\n",
        "    if categories_list[idx].find(categories[i]) == -1:\n",
        "      cate_labels_list[i].append(int(0))\n",
        "    else:\n",
        "      cate_labels_list[i].append(int(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEzoBf6ErH3U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "96a767d5-5bd3-4795-99c8-0826594924b8"
      },
      "source": [
        "train = pd.DataFrame(idx_list,columns=['idx'])\n",
        "train['text_reply'] = text_reply_list\n",
        "for i in range(0,len(categories)):\n",
        "  train[categories[i]] = cate_labels_list[i]\n",
        "train.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>text_reply</th>\n",
              "      <th>\"agree\"</th>\n",
              "      <th>\"applause\"</th>\n",
              "      <th>\"awww\"</th>\n",
              "      <th>\"dance\"</th>\n",
              "      <th>\"deal_with_it\"</th>\n",
              "      <th>\"do_not_want\"</th>\n",
              "      <th>\"eww\"</th>\n",
              "      <th>\"eye_roll\"</th>\n",
              "      <th>\"facepalm\"</th>\n",
              "      <th>\"fist_bump\"</th>\n",
              "      <th>\"good_luck\"</th>\n",
              "      <th>\"happy_dance\"</th>\n",
              "      <th>\"hearts\"</th>\n",
              "      <th>\"high_five\"</th>\n",
              "      <th>\"hug\"</th>\n",
              "      <th>\"idk\"</th>\n",
              "      <th>\"kiss\"</th>\n",
              "      <th>\"mic_drop\"</th>\n",
              "      <th>\"no\"</th>\n",
              "      <th>\"oh_snap\"</th>\n",
              "      <th>\"ok\"</th>\n",
              "      <th>\"omg\"</th>\n",
              "      <th>\"oops\"</th>\n",
              "      <th>\"please\"</th>\n",
              "      <th>\"popcorn\"</th>\n",
              "      <th>\"scared\"</th>\n",
              "      <th>\"seriously\"</th>\n",
              "      <th>\"shocked\"</th>\n",
              "      <th>\"shrug\"</th>\n",
              "      <th>\"sigh\"</th>\n",
              "      <th>\"slow_clap\"</th>\n",
              "      <th>\"smh\"</th>\n",
              "      <th>\"sorry\"</th>\n",
              "      <th>\"thank_you\"</th>\n",
              "      <th>\"thumbs_down\"</th>\n",
              "      <th>\"thumbs_up\"</th>\n",
              "      <th>\"want\"</th>\n",
              "      <th>\"win\"</th>\n",
              "      <th>\"wink\"</th>\n",
              "      <th>\"yawn\"</th>\n",
              "      <th>\"yes\"</th>\n",
              "      <th>\"yolo\"</th>\n",
              "      <th>\"you_got_this\"</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>we can all agree that any song by Niall Horan....</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Will you be installing #ScottyFromMarketing's ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Growing up my mum would call me a Nigga despit...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Rest your head on my chest when the world feel...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Imagine Will Hernandez and Wills both doing a ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   idx  ... \"you_got_this\"\n",
              "0    0  ...              0\n",
              "1    1  ...              0\n",
              "2    2  ...              0\n",
              "3    3  ...              0\n",
              "4    4  ...              0\n",
              "\n",
              "[5 rows x 45 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qssS2H4LY0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ID = 'idx'\n",
        "DATA_COLUMN = 'text_reply'\n",
        "LABEL_COLUMNS = categories"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zdXeJN9Q7lV",
        "colab_type": "text"
      },
      "source": [
        "## Training Parameters Define and Data Pre-processing(to BERT format)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q68KziE8LdzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, labels=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            labels: (Optional) [string]. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.labels = labels\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_ids, is_real_example=True):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_ids = label_ids,\n",
        "        self.is_real_example=is_real_example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XYb7eOELg93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_examples(df, labels_available=True):\n",
        "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "    examples = []\n",
        "    for (i, row) in enumerate(df.values):\n",
        "        guid = row[0]\n",
        "        text_a = row[1]\n",
        "        if labels_available:\n",
        "            labels = row[2:]\n",
        "        else:\n",
        "            labels = [0 for i in range(43)]\n",
        "        examples.append(\n",
        "            InputExample(guid=guid, text_a=text_a, labels=labels))\n",
        "    return examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mxnzr1-SJiZ",
        "colab_type": "text"
      },
      "source": [
        "Divided the data into training set (90%) and testing set (10%) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GE-OVqULifP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_VAL_RATIO = 0.9\n",
        "LEN = train.shape[0]\n",
        "SIZE_TRAIN = int(TRAIN_VAL_RATIO*LEN)\n",
        "\n",
        "x_train = train[:SIZE_TRAIN]\n",
        "x_val = train[SIZE_TRAIN:]\n",
        "\n",
        "train_examples = create_examples(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMnVCs98Ljdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_examples_to_features(examples,  max_seq_length, tokenizer):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        print(example.text_a)\n",
        "        tokens_a = tokenizer.tokenize(example.text_a)\n",
        "\n",
        "        tokens_b = None\n",
        "        if example.text_b:\n",
        "            tokens_b = tokenizer.tokenize(example.text_b)\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "        else:\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > max_seq_length - 2:\n",
        "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "\n",
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids: 0   0   0   0  0     0 0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
        "        segment_ids = [0] * len(tokens)\n",
        "\n",
        "        if tokens_b:\n",
        "            tokens += tokens_b + [\"[SEP]\"]\n",
        "            segment_ids += [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "        \n",
        "        labels_ids = []\n",
        "        for label in example.labels:\n",
        "            labels_ids.append(int(label))\n",
        "\n",
        "        if ex_index < 0:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % (example.guid))\n",
        "            logger.info(\"tokens: %s\" % \" \".join(\n",
        "                    [str(x) for x in tokens]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "            logger.info(\n",
        "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "            logger.info(\"label: %s (id = %s)\" % (example.labels, labels_ids))\n",
        "\n",
        "        features.append(\n",
        "                InputFeatures(input_ids=input_ids,\n",
        "                              input_mask=input_mask,\n",
        "                              segment_ids=segment_ids,\n",
        "                              label_ids=labels_ids))\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX_1SGYHS5Xn",
        "colab_type": "text"
      },
      "source": [
        "Computing the max length of text_reply (parameter in BERT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRFooL5fLliU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "56238833-9351-405b-b9dd-d865752fe6d2"
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for text_reply in text_reply_list:\n",
        "\n",
        "\n",
        "    input_ids = tokenizer.tokenize(text_reply)\n",
        "\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "    # print(len(input_ids))\n",
        "    \n",
        "\n",
        "print('Max text length: ', max_len)\n",
        "\n",
        "MAX_SEQ_LENGTH = max_len + 10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max text length:  264\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHdlWCapTDkQ",
        "colab_type": "text"
      },
      "source": [
        "**Training Parameters !!!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx770Dr5Ln3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 3e-5\n",
        "NUM_TRAIN_EPOCHS = 8.0\n",
        "WARMUP_PROPORTION = 0.1\n",
        "SAVE_CHECKPOINTS_STEPS = 1000\n",
        "SAVE_SUMMARY_STEPS = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAKQPn6gLpE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PaddingInputExample(object):\n",
        "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
        "    When running eval/predict on the TPU, we need to pad the number of examples\n",
        "    to be a multiple of the batch size, because the TPU requires a fixed batch\n",
        "    size. The alternative is to drop the last batch, which is bad because it means\n",
        "    the entire output data won't be generated.\n",
        "    We use this class instead of `None` because treating `None` as padding\n",
        "    battches could cause silent errors.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "def convert_single_example(ex_index, example, max_seq_length,\n",
        "                           tokenizer):\n",
        "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "\n",
        "    if isinstance(example, PaddingInputExample):\n",
        "        return InputFeatures(\n",
        "            input_ids=[0] * max_seq_length,\n",
        "            input_mask=[0] * max_seq_length,\n",
        "            segment_ids=[0] * max_seq_length,\n",
        "            label_ids=0,\n",
        "            is_real_example=False)\n",
        "\n",
        "    tokens_a = tokenizer.tokenize(example.text_a)\n",
        "    tokens_b = None\n",
        "    if example.text_b:\n",
        "        tokens_b = tokenizer.tokenize(example.text_b)\n",
        "\n",
        "    if tokens_b:\n",
        "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "        # length is less than the specified length.\n",
        "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "    else:\n",
        "        # Account for [CLS] and [SEP] with \"- 2\"\n",
        "        if len(tokens_a) > max_seq_length - 2:\n",
        "            tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
        "\n",
        "    # The convention in BERT is:\n",
        "    # (a) For sequence pairs:\n",
        "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "    #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
        "    # (b) For single sequences:\n",
        "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "    #  type_ids: 0     0   0   0  0     0 0\n",
        "    #\n",
        "    # Where \"type_ids\" are used to indicate whether this is the first\n",
        "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "    # embedding vector (and position vector). This is not *strictly* necessary\n",
        "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "    # it easier for the model to learn the concept of sequences.\n",
        "    #\n",
        "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "    # used as the \"sentence vector\". Note that this only makes sense because\n",
        "    # the entire model is fine-tuned.\n",
        "    tokens = []\n",
        "    segment_ids = []\n",
        "    tokens.append(\"[CLS]\")\n",
        "    segment_ids.append(0)\n",
        "    for token in tokens_a:\n",
        "        tokens.append(token)\n",
        "        segment_ids.append(0)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(0)\n",
        "\n",
        "    if tokens_b:\n",
        "        for token in tokens_b:\n",
        "            tokens.append(token)\n",
        "            segment_ids.append(1)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        segment_ids.append(1)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    while len(input_ids) < max_seq_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "        segment_ids.append(0)\n",
        "\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "\n",
        "    labels_ids = []\n",
        "    for label in example.labels:\n",
        "        labels_ids.append(int(label))\n",
        "\n",
        "\n",
        "    feature = InputFeatures(\n",
        "        input_ids=input_ids,\n",
        "        input_mask=input_mask,\n",
        "        segment_ids=segment_ids,\n",
        "        label_ids=labels_ids,\n",
        "        is_real_example=True)\n",
        "    return feature\n",
        "\n",
        "\n",
        "def file_based_convert_examples_to_features(\n",
        "        examples, max_seq_length, tokenizer, output_file):\n",
        "    \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n",
        "\n",
        "    writer = tf.python_io.TFRecordWriter(output_file)\n",
        "\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        #if ex_index % 10000 == 0:\n",
        "            #tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
        "\n",
        "        feature = convert_single_example(ex_index, example,\n",
        "                                         max_seq_length, tokenizer)\n",
        "\n",
        "        def create_int_feature(values):\n",
        "            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
        "            return f\n",
        "\n",
        "        features = collections.OrderedDict()\n",
        "        features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
        "        features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
        "        features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
        "        features[\"is_real_example\"] = create_int_feature(\n",
        "            [int(feature.is_real_example)])\n",
        "        if isinstance(feature.label_ids, list):\n",
        "            label_ids = feature.label_ids\n",
        "        else:\n",
        "            label_ids = feature.label_ids[0]\n",
        "        features[\"label_ids\"] = create_int_feature(label_ids)\n",
        "\n",
        "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
        "        writer.write(tf_example.SerializeToString())\n",
        "    writer.close()\n",
        "\n",
        "\n",
        "def file_based_input_fn_builder(input_file, seq_length, is_training,\n",
        "                                drop_remainder):\n",
        "    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "    name_to_features = {\n",
        "        \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "        \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "        \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "        \"label_ids\": tf.FixedLenFeature([43], tf.int64),\n",
        "        \"is_real_example\": tf.FixedLenFeature([], tf.int64),\n",
        "    }\n",
        "\n",
        "    def _decode_record(record, name_to_features):\n",
        "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
        "        example = tf.parse_single_example(record, name_to_features)\n",
        "\n",
        "        # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
        "        # So cast all int64 to int32.\n",
        "        for name in list(example.keys()):\n",
        "            t = example[name]\n",
        "            if t.dtype == tf.int64:\n",
        "                t = tf.to_int32(t)\n",
        "            example[name] = t\n",
        "\n",
        "        return example\n",
        "\n",
        "    def input_fn(params):\n",
        "        \"\"\"The actual input function.\"\"\"\n",
        "        batch_size = params[\"batch_size\"]\n",
        "\n",
        "        # For training, we want a lot of parallel reading and shuffling.\n",
        "        # For eval, we want no shuffling and parallel reading doesn't matter.\n",
        "        d = tf.data.TFRecordDataset(input_file)\n",
        "        if is_training:\n",
        "            d = d.repeat()\n",
        "            d = d.shuffle(buffer_size=100)\n",
        "\n",
        "        d = d.apply(\n",
        "            tf.contrib.data.map_and_batch(\n",
        "                lambda record: _decode_record(record, name_to_features),\n",
        "                batch_size=batch_size,\n",
        "                drop_remainder=drop_remainder))\n",
        "\n",
        "        return d\n",
        "\n",
        "    return input_fn\n",
        "\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnwCwpCHLtOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute # train and warmup steps from batch size\n",
        "num_train_steps = int(len(train_examples) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ubr3KB_iLuvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file = os.path.join('./working/', \"train.tf_record\")\n",
        "if not os.path.exists(train_file):\n",
        "    open(train_file, 'w').close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-m9x4COLv7o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "6087dc9b-fb6c-4917-da72-e86605790dc9"
      },
      "source": [
        "file_based_convert_examples_to_features(\n",
        "            train_examples, MAX_SEQ_LENGTH, tokenizer, train_file)\n",
        "tf.logging.info(\"***** Running training *****\")\n",
        "tf.logging.info(\"  Num examples = %d\", len(train_examples))\n",
        "tf.logging.info(\"  Batch size = %d\", BATCH_SIZE)\n",
        "tf.logging.info(\"  Num steps = %d\", num_train_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:***** Running training *****\n",
            "INFO:tensorflow:  Num examples = 28800\n",
            "INFO:tensorflow:  Batch size = 16\n",
            "INFO:tensorflow:  Num steps = 14400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quij_0daLxh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_input_fn = file_based_input_fn_builder(\n",
        "    input_file=train_file,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpN2OLssT50b",
        "colab_type": "text"
      },
      "source": [
        "## BERT Model configuration and Creating Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz0_u5tzLy1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 labels, num_labels, use_one_hot_embeddings):\n",
        "    \"\"\"Creates a classification model.\"\"\"\n",
        "    model = modeling.BertModel(\n",
        "        config=bert_config,\n",
        "        is_training=is_training,\n",
        "        input_ids=input_ids,\n",
        "        input_mask=input_mask,\n",
        "        token_type_ids=segment_ids,\n",
        "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "    # In the demo, we are doing a simple classification task on the entire\n",
        "    # segment.\n",
        "    #\n",
        "    # If you want to use the token-level output, use model.get_sequence_output()\n",
        "    # instead.\n",
        "    output_layer = model.get_pooled_output()\n",
        "\n",
        "    hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "    output_weights = tf.get_variable(\n",
        "        \"output_weights\", [num_labels, hidden_size],\n",
        "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "    output_bias = tf.get_variable(\n",
        "        \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "    with tf.variable_scope(\"loss\"):\n",
        "        if is_training:\n",
        "            # I.e., 0.1 dropout\n",
        "            output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "        logits = tf.nn.bias_add(logits, output_bias)\n",
        "        \n",
        "        # probabilities = tf.nn.softmax(logits, axis=-1) ### multiclass case\n",
        "        probabilities = tf.nn.sigmoid(logits)#### multi-label case\n",
        "        \n",
        "        labels = tf.cast(labels, tf.float32)\n",
        "        tf.logging.info(\"num_labels:{};logits:{};labels:{}\".format(num_labels, logits, labels))\n",
        "        per_example_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)\n",
        "        loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "        # probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "        # log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "        #\n",
        "        # one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "        #\n",
        "        # per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "        # loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "        return (loss, per_example_loss, logits, probabilities)\n",
        "\n",
        "\n",
        "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n",
        "                     num_train_steps, num_warmup_steps, use_tpu,\n",
        "                     use_one_hot_embeddings):\n",
        "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "        tf.logging.info(\"*** Features ***\")\n",
        "        for name in sorted(features.keys()):\n",
        "           tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "        input_ids = features[\"input_ids\"]\n",
        "        input_mask = features[\"input_mask\"]\n",
        "        segment_ids = features[\"segment_ids\"]\n",
        "        label_ids = features[\"label_ids\"]\n",
        "        is_real_example = None\n",
        "        if \"is_real_example\" in features:\n",
        "             is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
        "        else:\n",
        "             is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
        "\n",
        "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "        (total_loss, per_example_loss, logits, probabilities) = create_model(\n",
        "            bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
        "            num_labels, use_one_hot_embeddings)\n",
        "\n",
        "        tvars = tf.trainable_variables()\n",
        "        initialized_variable_names = {}\n",
        "        scaffold_fn = None\n",
        "        if init_checkpoint:\n",
        "            (assignment_map, initialized_variable_names\n",
        "             ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "            if use_tpu:\n",
        "\n",
        "                def tpu_scaffold():\n",
        "                    tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "                    return tf.train.Scaffold()\n",
        "\n",
        "                scaffold_fn = tpu_scaffold\n",
        "            else:\n",
        "                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "        tf.logging.info(\"**** Trainable Variables ****\")\n",
        "        for var in tvars:\n",
        "            init_string = \"\"\n",
        "            if var.name in initialized_variable_names:\n",
        "                init_string = \", *INIT_FROM_CKPT*\"\n",
        "            #tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,init_string)\n",
        "\n",
        "        output_spec = None\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "            train_op = optimization.create_optimizer(\n",
        "                total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "            output_spec = tf.estimator.EstimatorSpec(\n",
        "                mode=mode,\n",
        "                loss=total_loss,\n",
        "                train_op=train_op,\n",
        "                scaffold=scaffold_fn)\n",
        "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "\n",
        "            def metric_fn(per_example_loss, label_ids, probabilities, is_real_example):\n",
        "\n",
        "                logits_split = tf.split(probabilities, num_labels, axis=-1)\n",
        "                label_ids_split = tf.split(label_ids, num_labels, axis=-1)\n",
        "                # metrics change to auc of every class\n",
        "                eval_dict = {}\n",
        "                for j, logits in enumerate(logits_split):\n",
        "                    label_id_ = tf.cast(label_ids_split[j], dtype=tf.int32)\n",
        "                    current_auc, update_op_auc = tf.metrics.auc(label_id_, logits)\n",
        "                    eval_dict[str(j)] = (current_auc, update_op_auc)\n",
        "                eval_dict['eval_loss'] = tf.metrics.mean(values=per_example_loss)\n",
        "                return eval_dict\n",
        "\n",
        "                ## original eval metrics\n",
        "                # predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "                # accuracy = tf.metrics.accuracy(\n",
        "                #     labels=label_ids, predictions=predictions, weights=is_real_example)\n",
        "                # loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
        "                # return {\n",
        "                #     \"eval_accuracy\": accuracy,\n",
        "                #     \"eval_loss\": loss,\n",
        "                # }\n",
        "\n",
        "            eval_metrics = metric_fn(per_example_loss, label_ids, probabilities, is_real_example)\n",
        "            output_spec = tf.estimator.EstimatorSpec(\n",
        "                mode=mode,\n",
        "                loss=total_loss,\n",
        "                eval_metric_ops=eval_metrics,\n",
        "                scaffold=scaffold_fn)\n",
        "        else:\n",
        "            print(\"mode:\", mode,\"probabilities:\", probabilities)\n",
        "            output_spec = tf.estimator.EstimatorSpec(\n",
        "                mode=mode,\n",
        "                predictions={\"probabilities\": probabilities},\n",
        "                scaffold=scaffold_fn)\n",
        "        return output_spec\n",
        "\n",
        "    return model_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQy6ZxrmL09L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OUTPUT_DIR = \"./working/output\"\n",
        "# Specify outpit directory and number of checkpoint steps to save\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    keep_checkpoint_max=1,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCeEnBOnL36V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "365cb707-70f2-4e54-c191-12e160396ebd"
      },
      "source": [
        "bert_config = modeling.BertConfig.from_json_file(BERT_CONFIG)\n",
        "model_fn = model_fn_builder(\n",
        "  bert_config=bert_config,\n",
        "  num_labels= len(LABEL_COLUMNS),\n",
        "  init_checkpoint=BERT_INIT_CHKPNT,\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps,\n",
        "  use_tpu=False,\n",
        "  use_one_hot_embeddings=False)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "  model_fn=model_fn,\n",
        "  config=run_config,\n",
        "  params={\"batch_size\": BATCH_SIZE})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': './working/output', '_tf_random_seed': None, '_save_summary_steps': 500, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f444f0b1cc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwfMhLVCUnw5",
        "colab_type": "text"
      },
      "source": [
        "## Training and Evaluation !!!!!!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOLozJHWL5zT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "196b6385-f53f-4b41-ee51-fc846b79499f"
      },
      "source": [
        "print(f'Beginning Training!')\n",
        "current_time = datetime.now()\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Training took time \", datetime.now() - current_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning Training!\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-14-ff18378ccf1a>:179: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/data/python/ops/batching.py:276: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-14-ff18378ccf1a>:159: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:*** Features ***\n",
            "INFO:tensorflow:  name = input_ids, shape = (16, 274)\n",
            "INFO:tensorflow:  name = input_mask, shape = (16, 274)\n",
            "INFO:tensorflow:  name = is_real_example, shape = (16,)\n",
            "INFO:tensorflow:  name = label_ids, shape = (16, 43)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (16, 274)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "INFO:tensorflow:num_labels:43;logits:Tensor(\"loss/BiasAdd:0\", shape=(16, 43), dtype=float32);labels:Tensor(\"loss/Cast:0\", shape=(16, 43), dtype=float32)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "INFO:tensorflow:**** Trainable Variables ****\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into ./working/output/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.71014315, step = 0\n",
            "INFO:tensorflow:global_step/sec: 0.956702\n",
            "INFO:tensorflow:loss = 0.3266203, step = 100 (104.527 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.971308\n",
            "INFO:tensorflow:loss = 0.19863805, step = 200 (102.954 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.971215\n",
            "INFO:tensorflow:loss = 0.16984287, step = 300 (102.964 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.972268\n",
            "INFO:tensorflow:loss = 0.13880603, step = 400 (102.852 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.937035\n",
            "INFO:tensorflow:loss = 0.12186509, step = 500 (106.720 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.972969\n",
            "INFO:tensorflow:loss = 0.12409616, step = 600 (102.778 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.971489\n",
            "INFO:tensorflow:loss = 0.1347259, step = 700 (102.935 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.97008\n",
            "INFO:tensorflow:loss = 0.15766524, step = 800 (103.084 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.972204\n",
            "INFO:tensorflow:loss = 0.18489763, step = 900 (102.859 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1000 into ./working/output/model.ckpt.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "INFO:tensorflow:global_step/sec: 0.922459\n",
            "INFO:tensorflow:loss = 0.12652276, step = 1000 (108.406 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.967841\n",
            "INFO:tensorflow:loss = 0.1295717, step = 1100 (103.323 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.970947\n",
            "INFO:tensorflow:loss = 0.13303949, step = 1200 (102.992 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.972068\n",
            "INFO:tensorflow:loss = 0.14482772, step = 1300 (102.874 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.970515\n",
            "INFO:tensorflow:loss = 0.1501091, step = 1400 (103.038 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.970699\n",
            "INFO:tensorflow:loss = 0.115655474, step = 1500 (103.018 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.970982\n",
            "INFO:tensorflow:loss = 0.14335428, step = 1600 (102.989 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.970529\n",
            "INFO:tensorflow:loss = 0.15858951, step = 1700 (103.037 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.96995\n",
            "INFO:tensorflow:loss = 0.13781005, step = 1800 (103.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.971901\n",
            "INFO:tensorflow:loss = 0.12297038, step = 1900 (102.891 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2000 into ./working/output/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.921586\n",
            "INFO:tensorflow:loss = 0.15887836, step = 2000 (108.508 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.96752\n",
            "INFO:tensorflow:loss = 0.1388769, step = 2100 (103.358 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.969467\n",
            "INFO:tensorflow:loss = 0.15464877, step = 2200 (103.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.969485\n",
            "INFO:tensorflow:loss = 0.12797141, step = 2300 (103.148 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.970228\n",
            "INFO:tensorflow:loss = 0.17750412, step = 2400 (103.069 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.970429\n",
            "INFO:tensorflow:loss = 0.15209374, step = 2500 (103.047 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.969278\n",
            "INFO:tensorflow:loss = 0.1367345, step = 2600 (103.170 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.971163\n",
            "INFO:tensorflow:loss = 0.12964992, step = 2700 (102.970 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.968588\n",
            "INFO:tensorflow:loss = 0.12209651, step = 2800 (103.243 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.968042\n",
            "INFO:tensorflow:loss = 0.135291, step = 2900 (103.302 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 3000 into ./working/output/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.920762\n",
            "INFO:tensorflow:loss = 0.098393306, step = 3000 (108.605 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.968085\n",
            "INFO:tensorflow:loss = 0.14771472, step = 3100 (103.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.969905\n",
            "INFO:tensorflow:loss = 0.16217, step = 3200 (103.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.969189\n",
            "INFO:tensorflow:loss = 0.14272538, step = 3300 (103.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.971914\n",
            "INFO:tensorflow:loss = 0.11546468, step = 3400 (102.890 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.970424\n",
            "INFO:tensorflow:loss = 0.14375493, step = 3500 (103.048 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.96933\n",
            "INFO:tensorflow:loss = 0.105570026, step = 3600 (103.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.968908\n",
            "INFO:tensorflow:loss = 0.15374047, step = 3700 (103.209 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.969713\n",
            "INFO:tensorflow:loss = 0.14791387, step = 3800 (103.123 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.969813\n",
            "INFO:tensorflow:loss = 0.13015991, step = 3900 (103.114 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 4000 into ./working/output/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.918025\n",
            "INFO:tensorflow:loss = 0.13773538, step = 4000 (108.928 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.967231\n",
            "INFO:tensorflow:loss = 0.11047043, step = 4100 (103.388 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.971983\n",
            "INFO:tensorflow:loss = 0.17638364, step = 4200 (102.882 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.971519\n",
            "INFO:tensorflow:loss = 0.121164076, step = 4300 (102.932 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.969673\n",
            "INFO:tensorflow:loss = 0.13103579, step = 4400 (103.127 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.968218\n",
            "INFO:tensorflow:loss = 0.13774778, step = 4500 (103.283 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.969801\n",
            "INFO:tensorflow:loss = 0.11108855, step = 4600 (103.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.969518\n",
            "INFO:tensorflow:loss = 0.110997275, step = 4700 (103.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.968241\n",
            "INFO:tensorflow:loss = 0.11332942, step = 4800 (103.280 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.968223\n",
            "INFO:tensorflow:loss = 0.15105821, step = 4900 (103.282 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 5000 into ./working/output/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.919502\n",
            "INFO:tensorflow:loss = 0.14435604, step = 5000 (108.754 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.966449\n",
            "INFO:tensorflow:loss = 0.105861306, step = 5100 (103.472 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.970271\n",
            "INFO:tensorflow:loss = 0.12990813, step = 5200 (103.063 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.967942\n",
            "INFO:tensorflow:loss = 0.13193944, step = 5300 (103.312 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.968957\n",
            "INFO:tensorflow:loss = 0.11055476, step = 5400 (103.204 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.969447\n",
            "INFO:tensorflow:loss = 0.16092001, step = 5500 (103.152 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.971616\n",
            "INFO:tensorflow:loss = 0.11853265, step = 5600 (102.922 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.969048\n",
            "INFO:tensorflow:loss = 0.10752466, step = 5700 (103.194 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.96929\n",
            "INFO:tensorflow:loss = 0.12597586, step = 5800 (103.169 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.97031\n",
            "INFO:tensorflow:loss = 0.10517568, step = 5900 (103.059 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 6000 into ./working/output/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.919261\n",
            "INFO:tensorflow:loss = 0.15651268, step = 6000 (108.783 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.967056\n",
            "INFO:tensorflow:loss = 0.12469254, step = 6100 (103.407 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.969726\n",
            "INFO:tensorflow:loss = 0.12054314, step = 6200 (103.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.969654\n",
            "INFO:tensorflow:loss = 0.1412747, step = 6300 (103.130 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.972094\n",
            "INFO:tensorflow:loss = 0.11498009, step = 6400 (102.870 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.969848\n",
            "INFO:tensorflow:loss = 0.14235216, step = 6500 (103.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.970984\n",
            "INFO:tensorflow:loss = 0.10526412, step = 6600 (102.989 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.969772\n",
            "INFO:tensorflow:loss = 0.13211946, step = 6700 (103.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.97256\n",
            "INFO:tensorflow:loss = 0.13061073, step = 6800 (102.821 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.970384\n",
            "INFO:tensorflow:loss = 0.13204199, step = 6900 (103.053 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 7000 into ./working/output/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.919457\n",
            "INFO:tensorflow:loss = 0.11905586, step = 7000 (108.759 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.966782\n",
            "INFO:tensorflow:loss = 0.124887064, step = 7100 (103.437 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.972266\n",
            "INFO:tensorflow:loss = 0.11876124, step = 7200 (102.852 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.969846\n",
            "INFO:tensorflow:loss = 0.11918325, step = 7300 (103.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.969936\n",
            "INFO:tensorflow:loss = 0.11008558, step = 7400 (103.099 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.970712\n",
            "INFO:tensorflow:loss = 0.15058112, step = 7500 (103.017 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.969522\n",
            "INFO:tensorflow:loss = 0.12873581, step = 7600 (103.144 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.973294\n",
            "INFO:tensorflow:loss = 0.1451615, step = 7700 (102.744 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.9686\n",
            "INFO:tensorflow:loss = 0.09690941, step = 7800 (103.242 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.970495\n",
            "INFO:tensorflow:loss = 0.102053195, step = 7900 (103.041 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 8000 into ./working/output/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.920583\n",
            "INFO:tensorflow:loss = 0.11836389, step = 8000 (108.626 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.967258\n",
            "INFO:tensorflow:loss = 0.1302576, step = 8100 (103.386 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.970885\n",
            "INFO:tensorflow:loss = 0.099405155, step = 8200 (102.999 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.968689\n",
            "INFO:tensorflow:loss = 0.13000284, step = 8300 (103.232 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.969176\n",
            "INFO:tensorflow:loss = 0.09442865, step = 8400 (103.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.969712\n",
            "INFO:tensorflow:loss = 0.1439485, step = 8500 (103.123 sec)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nKQtm6EL7kP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_file = os.path.join('./working', \"eval.tf_record\")\n",
        "if not os.path.exists(eval_file):\n",
        "    open(eval_file, 'w').close()\n",
        "\n",
        "eval_examples = create_examples(x_val)\n",
        "file_based_convert_examples_to_features(\n",
        "    eval_examples, MAX_SEQ_LENGTH, tokenizer, eval_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meiF6-UIL9Lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This tells the estimator to run through the entire set.\n",
        "eval_steps = None\n",
        "\n",
        "eval_drop_remainder = False\n",
        "eval_input_fn = file_based_input_fn_builder(\n",
        "    input_file=eval_file,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)\n",
        "\n",
        "result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-_nxY6gL-oA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_eval_file = os.path.join(\"./working\", \"eval_results.txt\")\n",
        "with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "    tf.logging.info(\"***** Eval results *****\")\n",
        "    for key in sorted(result.keys()):\n",
        "        tf.logging.info(\"  %s = %s\", key, str(result[key]))\n",
        "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_VQoNeZVCqk",
        "colab_type": "text"
      },
      "source": [
        "## Fetch the eval data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsMm2lxSMBDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2 = pd.read_csv(\"https://nctu_team_yellow.imfast.io/nlp_final_project/test_unlabeled.csv\", delimiter=',', header=None, names=['idx', 'text', 'reply'])[1:]\n",
        "# df2 = pd.read_csv(\"https://nctu_team_yellow.imfast.io/nlp_final_project/dev_unlabeled.csv\", delimiter=',', header=None, names=['idx', 'text', 'reply'])\n",
        "eval_idx_list = df2.idx.tolist()\n",
        "eval_text_list = df2.text.tolist()\n",
        "eval_reply_list = df2.reply.tolist()\n",
        "\n",
        "eval_text_reply_list = eval_text_list\n",
        "for i in range(0,len(eval_text_list)):\n",
        "  if type(eval_reply_list[i]) == str:\n",
        "    eval_text_reply_list[i] = eval_text_list[i] + eval_reply_list[i]\n",
        "\n",
        "eval_csv = pd.DataFrame(eval_idx_list,columns=['idx'])\n",
        "eval_csv['text_reply'] = eval_text_reply_list\n",
        "eval_csv.head(5)\n",
        "\n",
        "\n",
        "x_test = eval_csv #testing a small sample\n",
        "x_test = x_test.reset_index(drop=True)\n",
        "predict_examples = create_examples(x_test,False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPf7HmHXMCMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_features = convert_examples_to_features(predict_examples, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlRP13c4MCqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_fn_builder(features, seq_length, is_training, drop_remainder):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  all_input_ids = []\n",
        "  all_input_mask = []\n",
        "  all_segment_ids = []\n",
        "  all_label_ids = []\n",
        "\n",
        "  for feature in features:\n",
        "    all_input_ids.append(feature.input_ids)\n",
        "    all_input_mask.append(feature.input_mask)\n",
        "    all_segment_ids.append(feature.segment_ids)\n",
        "    all_label_ids.append(feature.label_ids)\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    num_examples = len(features)\n",
        "\n",
        "    # This is for demo purposes and does NOT scale to large data sets. We do\n",
        "    # not use Dataset.from_generator() because that uses tf.py_func which is\n",
        "    # not TPU compatible. The right way to load data is with TFRecordReader.\n",
        "    d = tf.data.Dataset.from_tensor_slices({\n",
        "        \"input_ids\":\n",
        "            tf.constant(\n",
        "                all_input_ids, shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"input_mask\":\n",
        "            tf.constant(\n",
        "                all_input_mask,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"segment_ids\":\n",
        "            tf.constant(\n",
        "                all_segment_ids,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"label_ids\":\n",
        "            tf.constant(all_label_ids, shape=[num_examples, len(LABEL_COLUMNS)], dtype=tf.int32),\n",
        "    })\n",
        "\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=100)\n",
        "\n",
        "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
        "    return d\n",
        "\n",
        "  return input_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VECnac8wMEeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Beginning Predictions!')\n",
        "current_time = datetime.now()\n",
        "\n",
        "predict_input_fn = input_fn_builder(features=eval_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
        "predictions = estimator.predict(predict_input_fn)\n",
        "print(\"Prediction took time \", datetime.now() - current_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdHnKe9LMGNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_LABEL_COLUMNS = [\"agree\", \"applause\", \"awww\", \"dance\", \"deal_with_it\", \"do_not_want\", \"eww\", \"eye_roll\", \"facepalm\", \"fist_bump\", \"good_luck\", \"happy_dance\", \"hearts\", \"high_five\", \"hug\", \"idk\", \"kiss\", \"mic_drop\", \"no\", \"oh_snap\", \"ok\", \"omg\", \"oops\", \"please\", \"popcorn\", \"scared\", \"seriously\", \"shocked\", \"shrug\", \"sigh\", \"slow_clap\", \"smh\", \"sorry\", \"thank_you\", \"thumbs_down\", \"thumbs_up\", \"want\", \"win\", \"wink\", \"yawn\", \"yes\", \"yolo\", \"you_got_this\"]\n",
        "\n",
        "def create_output(predictions):\n",
        "    probabilities = []\n",
        "    for (i, prediction) in enumerate(predictions):\n",
        "        preds = prediction[\"probabilities\"]\n",
        "        probabilities.append(preds)\n",
        "    dff = pd.DataFrame(probabilities)\n",
        "    dff.columns = test_LABEL_COLUMNS\n",
        "    \n",
        "    return dff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leWh-iUZMHnL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_df = create_output(predictions)\n",
        "merged_df =  pd.concat([eval_csv, output_df], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8-KOeKMMIuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Row_lists =[] \n",
        "predict_cates_list = []\n",
        "# Iterate over each row \n",
        "for index, rows in merged_df.iterrows(): \n",
        "    # Create list for the current row \n",
        "    my_list =[rows.agree,rows.applause,rows.awww,rows.dance,rows.deal_with_it,rows.do_not_want,rows.eww,rows.eye_roll,rows.facepalm,rows.fist_bump,rows.good_luck,rows.happy_dance,rows.hearts,rows.high_five,rows.hug,rows.idk,rows.kiss,rows.mic_drop,rows.no,rows.oh_snap,rows.ok,rows.omg,rows.oops,rows.please,rows.popcorn,rows.scared,rows.seriously,rows.shocked,rows.shrug,rows.sigh,rows.slow_clap,rows.smh,rows.sorry,rows.thank_you,rows.thumbs_down,rows.thumbs_up,rows.want,rows.win,rows.wink,rows.yawn,rows.yes,rows.yolo,rows.you_got_this]    # append the list to the final list \n",
        "    Row_lists.append(my_list) \n",
        "\n",
        "for i in range(0,len(Row_lists)):\n",
        "  ranks = sorted( [(x,i) for (i,x) in enumerate(Row_lists[i])], reverse=True )\n",
        "  values = []\n",
        "  cate_indexs = []\n",
        "  predict_cates = []\n",
        "  for x,i in ranks:\n",
        "      if x not in values:\n",
        "          values.append( x )\n",
        "          cate_indexs.append( i )\n",
        "          if len(values) == 6:\n",
        "              break\n",
        "  predict_cates = [test_LABEL_COLUMNS[k] for k in cate_indexs]\n",
        "  # print(list(zip( values, predict_cates )))\n",
        "  print(predict_cates)\n",
        "  predict_cates_list.append(predict_cates)\n",
        "  print('---')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj0jnQsmFgYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = pd.read_csv(\"https://nctu_team_yellow.imfast.io/nlp_final_project/test_unlabeled.csv\", delimiter=',', header=None, names=['idx', 'text', 'reply'])[1:]\n",
        "tmp_reply = submission.reply.tolist()\n",
        "for (i,reply) in enumerate(tmp_reply):\n",
        "  if type(reply) != str:\n",
        "    tmp_reply[i] = ''\n",
        "tmp_idx = submission.idx.tolist()\n",
        "for (i,idx) in enumerate(tmp_idx):\n",
        "  tmp_idx[i] = int(tmp_idx[i])\n",
        "\n",
        "submission = submission[['text']]\n",
        "submission.insert(0,\"idx\",tmp_idx),\n",
        "submission.insert(1,\"categories\",predict_cates_list)\n",
        "submission.insert(2,\"reply\",tmp_reply)\n",
        "submission.to_json('./eval.json',orient='records',force_ascii=False,lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}