{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab2-0760054.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsanzxc456/NLP/blob/master/lab2_0760054.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPSD9EGUD1wf",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ga4-mr4oNLx",
        "colab_type": "text"
      },
      "source": [
        "## Fetching the Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH6SpNwAEKeE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "4fa3ebe5-0c9e-4cd2-f514-3d09f0f1767a"
      },
      "source": [
        "import string\n",
        "import pandas as pd\n",
        "!pip install Unidecode\n",
        "from unidecode import unidecode"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 30.1MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 4.7MB/s \n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeKjQYbSEvmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_corpus():\n",
        "  df = pd.read_csv('https://raw.githubusercontent.com/bshmueli/108-nlp/master/reuters.csv') # https://bit.ly/nlp-reuters\n",
        "  corpus = df.content.to_list()\n",
        "  for i in range(0,len(corpus)):\n",
        "    corpus[i] = unidecode(corpus[i])\n",
        "  return corpus"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrQe6L-ib8PM",
        "colab_type": "text"
      },
      "source": [
        "## NLTK tokenizers and POS tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "670jq-PG7Isu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "6302b6ec-f0b6-4bf6-83cb-b568834e678d"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "import time\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUF1uW_sbh7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doc_tokenizer(document):\n",
        "  token = nltk.word_tokenize(document)\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  word_list = []\n",
        "  token_pos = nltk.pos_tag(token)\n",
        "  for i in range(0,len(token_pos)):\n",
        "    if token_pos[i][1] == 'NNP':\n",
        "      word_list.append(token[i])\n",
        "  return word_list"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBmiR0VfgKJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vocab(corpus):\n",
        "  vocab_list = []\n",
        "  for i in range(0,len(corpus)):\n",
        "    tokens = doc_tokenizer(corpus[i])\n",
        "    vocab_list.extend(tokens)\n",
        "  return vocab_list"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajycj52GvFfu",
        "colab_type": "text"
      },
      "source": [
        "## Computing bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Pecuo4NvTLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "from nltk import ngrams"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8ioMBYVlAhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_most_5_bigrams(corpus):\n",
        "  bigrams_counter = Counter()\n",
        "  bigrams_counter.update(ngrams(get_vocab(corpus),2))\n",
        "  most_5_bigrams = bigrams_counter.most_common(5)\n",
        "  for i in range(0,len(most_5_bigrams)):\n",
        "    print('*{} >> ({})'.format(most_5_bigrams[i][0], most_5_bigrams[i][1]))\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6PuRwb52fGH",
        "colab_type": "text"
      },
      "source": [
        "## Testing !!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pydGDRVk2iDk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "f009bd33-5ac3-4a3d-8010-8b34dbc31727"
      },
      "source": [
        "corpus = get_corpus()\n",
        "get_most_5_bigrams(corpus)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*('U.', 'S.') >> (17601)\n",
            "*('Donald', 'Trump') >> (3222)\n",
            "*('New', 'York') >> (2443)\n",
            "*('Islamic', 'State') >> (1927)\n",
            "*('President', 'Donald') >> (1924)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMYDEVn_0i4v",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD3gacqDbO5-",
        "colab_type": "text"
      },
      "source": [
        "## Fetching the Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idHE30sexU34",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "afdc2070-88c3-4812-e43f-29880c6ddf7c"
      },
      "source": [
        "!pip install Unidecode\n",
        "import pandas as pd\n",
        "import string\n",
        "import spacy\n",
        "import time\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "from unidecode import unidecode"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHooH1ldwXO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_corpus():\n",
        "  df = pd.read_csv('https://raw.githubusercontent.com/bshmueli/108-nlp/master/buzzfeed.csv')\n",
        "  corpus = df.content.to_list()\n",
        "  for i in range(0,len(corpus)):\n",
        "    if type(corpus[i]) == type(0.1):\n",
        "      corpus[i] = \"hi\"\n",
        "    else:\n",
        "      corpus[i] = unidecode(corpus[i])\n",
        "  return corpus"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAMCPYAzwX5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_corpus_title():\n",
        "  df = pd.read_csv('https://raw.githubusercontent.com/bshmueli/108-nlp/master/buzzfeed.csv')\n",
        "  corpus_title = df.title.to_list()\n",
        "  return corpus_title"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSUhkMJ3xn_",
        "colab_type": "text"
      },
      "source": [
        "## Spacy tokenizers and POS tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQDk5LjZwZZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(document):\n",
        "  doc = nlp(document)\n",
        "  tokens = [token for token in doc if not token.is_stop]\n",
        "  tokens = [token for token in tokens if not token.is_punct]\n",
        "  tokens = [token for token in tokens if not token.is_space]\n",
        "  for i in range(0,len(tokens)):\n",
        "    tokens[i] = tokens[i].lemma_ + '_' + tokens[i].pos_\n",
        "  return tokens\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bm0cLBU31gl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OQtyK6XwbQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vocab_from_corpus(corpus):\n",
        "  vocabulary = Counter()\n",
        "  for i in range(0,len(corpus)):\n",
        "    tokens = tokenize(corpus[i])\n",
        "    vocabulary.update(tokens)\n",
        "  return vocabulary"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh9dfzM8wcOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vocab_from_document(document):\n",
        "  vocabulary = Counter()\n",
        "  tokens = tokenize(document)\n",
        "  vocabulary.update(tokens)\n",
        "  return vocabulary"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZk-0fMAxXNo",
        "colab_type": "text"
      },
      "source": [
        "## Computing TFIDF score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0DXGQ6fFvaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu6hQMGWTFEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_doc_vocab_list(corpus):\n",
        "    doc_vocab_list = []\n",
        "    for i in range(0,len(corpus)):\n",
        "      tmp_doc_vocab = get_vocab_from_document(corpus[i])\n",
        "      doc_vocab_list.append(tmp_doc_vocab)\n",
        "    return doc_vocab_list\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1-o4LY1wdVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_df(word_list):\n",
        "  df_list = []\n",
        "  for word in word_list:\n",
        "    tmp_df = 0\n",
        "    for i in range(0,len(corpus)):\n",
        "      if doc_vocab_list[i][word[0]]!=0:\n",
        "        tmp_df = tmp_df + 1\n",
        "    df_list.append(tmp_df)\n",
        "  return df_list"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjmePRvTwe9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tfidf_score(num,key_word,doc_id):\n",
        "  tf = doc_vocab_list[doc_id][key_word]/len(list(doc_vocab_list[doc_id].elements()))\n",
        "  idf = math.log10(len(corpus)/df_list[num])\n",
        "  tfidf = tf * idf\n",
        "  return tfidf"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzCPu6wBwf-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tfidf_vector(doc_id):\n",
        "  tfidf_vec = []\n",
        "  for i in range(0,len(top_512_vocab)):   \n",
        "    tfidf_vec.append(tfidf_score(i,top_512_vocab[i][0],doc_id))\n",
        "  return tfidf_vec"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kec4dNouwhJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine_similarity(vec_a, vec_b):\n",
        "  assert len(vec_a) == len(vec_b)\n",
        "  if sum(vec_a) == 0 or sum(vec_b) == 0:\n",
        "    return 0 # hack\n",
        "  a_b = sum(i[0] * i[1] for i in zip(vec_a, vec_b))\n",
        "  a_2 = sum([i*i for i in vec_a])\n",
        "  b_2 = sum([i*i for i in vec_b])\n",
        "  return a_b/(math.sqrt(a_2) * math.sqrt(b_2))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSkQQJb3wiWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doc_similarity(seed_vector, doc_id):\n",
        "  final_score = cosine_similarity(seed_vector, tfidf_vector(doc_id))\n",
        "  return final_score"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66B-cu_qwjja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def k_similar(seed_id, k):\n",
        "  seed_doc = corpus[seed_id]\n",
        "  seed_title = corpus_title[seed_id]\n",
        "  seed_tfidf_vector = tfidf_vector(seed_id)\n",
        "  print('> \"{}\"'.format(seed_title))\n",
        "  similarities = [doc_similarity(seed_tfidf_vector, doc_id) for id, doc_id in enumerate(range(0,len(corpus)))]\n",
        "  top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i])[-k:] # https://stackoverflow.com/questions/13070461/get-indices-of-the-top-n-values-of-a-list\n",
        "  nearest = [[corpus_title[id], similarities[id]] for id in top_indices]\n",
        "  print()\n",
        "  for story in reversed(nearest):\n",
        "    print('* \"{}\" ({})'.format(story[0], story[1]))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbah9jNSGhHc",
        "colab_type": "text"
      },
      "source": [
        "## Testing !!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB3wEmAuwkp9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "9bbbfd3a-6bf3-421f-fa7e-cdb655d3be91"
      },
      "source": [
        "corpus = get_corpus()\n",
        "corpus_title = get_corpus_title()\n",
        "top_512_vocab = get_vocab_from_corpus(corpus).most_common(512)\n",
        "doc_vocab_list = get_doc_vocab_list(corpus)\n",
        "df_list = get_df(top_512_vocab)\n",
        "k_similar(54,5)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> \"Clinton: No, I Do Have Young Enthusiastic Supporters\"\n",
            "\n",
            "* \"Clinton: No, I Do Have Young Enthusiastic Supporters\" (1.0)\n",
            "* \"Clinton Campaign Targets LGBT Voters In Push For Decisive New York Win\" (0.6138809125908788)\n",
            "* \"Why Bill Clinton’s Voters Wouldn’t Vote For Him Today, As Told By Bill Clinton\" (0.5496478797654698)\n",
            "* \"New Clinton Spanish-Language Ad Highlights “First-Time” Voters Opposing Trump\" (0.540856788882674)\n",
            "* \"Turning To The General, Clinton Makes Pitch To The “Thoughtful Republican”\" (0.5276332843231093)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
